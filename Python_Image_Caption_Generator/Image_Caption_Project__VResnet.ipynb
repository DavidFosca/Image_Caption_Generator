{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Image_Caption_Project _V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogv6jZXIssB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, RepeatVector, Embedding, LSTM, TimeDistributed, Concatenate\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_input_vgg\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet\n",
        "from copy import deepcopy\n",
        "import pickle\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5LvYMkZs6Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbEj4h2ZssB6",
        "colab_type": "text"
      },
      "source": [
        "### 1. Leemos la imágenes  y captions de Flickr8k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbT3jhhossB7",
        "colab_type": "text"
      },
      "source": [
        "Leemos el archivo de captions para obtener el 'id' de la imagen y sus descripciones. Por cada imagen se cuenta con 5 descripciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd_4r5IwssB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(filename):\n",
        "    content = \"\"\n",
        "    with open(filename) as f:\n",
        "        content = f.read().split(\"\\n\")\n",
        "    return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrwQaNaBssB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_size = (224,224,3)\n",
        "images_folder = '/gdrive/My Drive/dataset_imagecaption/Images/' #'flickr8k-dataset/images/'\n",
        "file_captions = '/gdrive/My Drive/dataset_imagecaption/captions.txt' #'flickr8k-dataset/captions.txt'\n",
        "\n",
        "#content = read_file('flickr8k-dataset/captions.txt')\n",
        "content = read_file(file_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_hdZ84HssCE",
        "colab_type": "text"
      },
      "source": [
        "Mostramos cuantas imágenes tiene nuestro dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoS77sYPssCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total de imágenes: \", len(os.listdir(images_folder)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_mGuw8FssCJ",
        "colab_type": "text"
      },
      "source": [
        "Como el archivo captions.txt tiene una estructura de la siguiente manera\n",
        "\n",
        "```\n",
        "image,caption\n",
        "\"1000268201_693b08cb0e.jpg\",\"A child in a pink dress is climbing up a set...\"\n",
        "\"1000268201_693b08cb0e.jpg\",\"A girl going into a wooden building...\"\n",
        "```\n",
        "\n",
        "Necesitamos leer el archivo linea por linea para obtener cada imagen y sus 5 descripciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOnVQqTBssCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset_content(content):\n",
        "    headers = content[0].split(',')\n",
        "    images_map = {}\n",
        "    images_with_captions = []\n",
        "    for idx in range(1, len(content)-1):\n",
        "        row = content[idx].split('.jpg')\n",
        "        image_id, caption = row[0] + '.jpg', row[1].strip()[1:]\n",
        "        images_with_captions.append([image_id, caption])\n",
        "        # Almacenamos en un mapa\n",
        "        if image_id not in images_map:\n",
        "            images_map[image_id] = []\n",
        "        images_map[image_id].append(caption)\n",
        "    return headers, images_with_captions, images_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdW0WZhqssCP",
        "colab_type": "text"
      },
      "source": [
        "Almacenamos los headers y data para el DataFrame. Además obtenemos un mapa cuya `key` es el id de la imagen y el valor es una lista de sus captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJJyTFOUssCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headers, images_with_captions, images_map = create_dataset_content(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBlMu1UpssCU",
        "colab_type": "text"
      },
      "source": [
        "Almacenamos todas las imágenes y los captions respecitvos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-xpYaGkssCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.DataFrame(data=images_with_captions, columns=headers)\n",
        "dataset['image'] = dataset['image'].apply(lambda image: image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyl7Mzd-8Wq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del images_with_captions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQLaUhBCssCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total de captions:\", dataset.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKpprAsCssCc",
        "colab_type": "text"
      },
      "source": [
        "### 2. Visualizamos algunas imágenes y sus captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgwdLWAjssCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images_with_captions(images_ids, dataset, num_images=3, target_size=(224,224,3)):\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    count = 1\n",
        "    for idx, image_id in enumerate(images_ids[:num_images]):\n",
        "        # Obtenemos los captions\n",
        "        captions = dataset[dataset['image'] == image_id]['caption'].values\n",
        "\n",
        "        #Obtenemos la imagen\n",
        "        img_path = f'{images_folder}{image_id}'\n",
        "        img = load_img(img_path, target_size=target_size)\n",
        "        img = img_to_array(img)\n",
        "\n",
        "        # Agregamos la imagen al plot\n",
        "        ax = fig.add_subplot(num_images, 2, count, xticks=[], yticks=[])\n",
        "        ax.imshow(img/255.)\n",
        "        count += 1\n",
        "\n",
        "        ax = fig.add_subplot(num_images,2, count)\n",
        "        plt.axis('off')\n",
        "        ax.plot()\n",
        "        ax.set_xlim(0,1)\n",
        "        ax.set_ylim(0,len(captions))\n",
        "        for i, caption in enumerate(captions):\n",
        "            ax.text(0,i, caption)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AUZakHwssCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_images_with_captions(list(images_map.keys()), dataset, num_images=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QZljRPHssCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_image(path, image_id, target_size=(224,224,3)):\n",
        "    img_path = f'{path}{image_id}'\n",
        "    img = load_img(img_path, target_size=target_size)\n",
        "    img = img_to_array(img) / 255.\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKuCsTyhssCo",
        "colab_type": "text"
      },
      "source": [
        "### 3. Creamos el modelo de CNN (Resnet50)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNBbuHgrssCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_vgg = VGG16(include_top=True)\n",
        "#model_vgg16 = Model(inputs=model_vgg.inputs, outputs=model_vgg.layers[-2].output)\n",
        "model_resnet = ResNet50(include_top=False, weights='imagenet', input_shape=target_size, pooling='avg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvy8D-1dssCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_resnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M19zBnDmssCu",
        "colab_type": "text"
      },
      "source": [
        "### 4. Extraemos los features vectors de cada imagen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDqvM2pQssCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_images_vgg(path, image_id, target_size=(224,224,3)):\n",
        "    img = load_img(f'{path}{image_id}', target_size=target_size)\n",
        "    img = img_to_array(img)\n",
        "    return preprocess_input_vgg(np.expand_dims(img, axis=0))\n",
        "\n",
        "def preprocess_images_resnet(path, image_id, target_size=(224,224,3)):\n",
        "    img = load_img(f'{path}{image_id}',target_size=target_size)\n",
        "    img = img_to_array(img)\n",
        "    return preprocess_input_resnet(np.expand_dims(img, axis=0))\n",
        "\n",
        "def extract_features(path, images, model, target_size, preprocessing):\n",
        "    images_features_map = {}\n",
        "    for image_id in tqdm(images):\n",
        "        img = preprocessing(path, image_id, target_size)\n",
        "        features = model.predict(img)\n",
        "        images_features_map[image_id] = features.reshape(2048)\n",
        "    return images_features_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDRgbgqMssCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do = False\n",
        "if do:\n",
        "    features_map = extract_features(images_folder, list(images_map.keys()), model_resnet, target_size, preprocess_images_resnet)\n",
        "    with open( \"precomputed/encoded_images.p\", \"wb\" ) as pickle_f:\n",
        "        pickle.dump(features_map, pickle_f) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qTRkzrsssC3",
        "colab_type": "text"
      },
      "source": [
        "### 5. Pre procesamiento de captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVqyBvqTssC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUEFvRz3ssC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    \"\"\" Removemos los signos de puntuación de cada plabra.\n",
        "        Se extiende con caracteres para español\n",
        "    \"\"\"\n",
        "    punctuation = string.punctuation + '¿¡'\n",
        "    return text.translate(str.maketrans('','', punctuation))\n",
        "\n",
        "def remove_short_words(text):\n",
        "    \"\"\" Remueve caracteres únicos. Por ejemplo ['a', 'c', 'd']\n",
        "    \"\"\"\n",
        "    words = text.split(' ')\n",
        "    return ' '.join([word for word in words if len(word) > 1 ])\n",
        "\n",
        "def remove_alpha_numeric(text):\n",
        "    \"\"\" Removemos aquellas palabras que contienen números o caracteres especiales\n",
        "    \"\"\"\n",
        "    words = text.split(' ')\n",
        "    return ' '.join([word for word in words if word.isalpha()])\n",
        "\n",
        "def preprocess_captions(text):\n",
        "    \"\"\" Pipeline para realizar el preprocesamiento del texto\n",
        "    \"\"\"\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_short_words(text)\n",
        "    text = remove_alpha_numeric(text)\n",
        "    return text.lower()\n",
        "\n",
        "def add_start_end_token(text):\n",
        "    \"\"\" Agregamos los tokens <start> <end> que se usarán en la secuencia\n",
        "    \"\"\"\n",
        "    return '<start> '+text+' <end>'\n",
        "\n",
        "def create_word_to_index(vocabulary):\n",
        "    #unique_words = set([word for sentence in captions for word in sentence.split()])\n",
        "    word_to_index = { word : idx for idx, word in enumerate(['<unk>'] + list(vocabulary)) }\n",
        "    return word_to_index\n",
        "    \n",
        "def create_index_to_word(vocabulary):\n",
        "    #unique_words = set([word for sentence in captions for word in sentence.split()])\n",
        "    index_to_word = { idx  : word for idx, word in enumerate(['<unk>'] + list(vocabulary)) }\n",
        "    return index_to_word\n",
        "\n",
        "def get_vocabulary(captions):\n",
        "    \"\"\" Obtenemos el vocabulario en base a los captions\n",
        "    \"\"\"\n",
        "    return set([word for sentence in captions for word in sentence.split()])\n",
        "\n",
        "def get_vocabulary_size(vocabulary):\n",
        "    \"\"\" Obtenemos el tamaño del vocabulario (+1 por el token de <unk>)\n",
        "    \"\"\"\n",
        "    return len(vocabulary)+1\n",
        "\n",
        "def get_max_caption_size(captions):\n",
        "    return max([len(sentence.split()) for sentence in captions])\n",
        "\n",
        "def create_sequences(dataset, word_to_index, vocab_size):\n",
        "    all_padded_sequences, all_subsequence_words = [], []\n",
        "    for idx in range(len(dataset)):\n",
        "        padded_sequence = []\n",
        "        next_words = []\n",
        "        w2i = [word_to_index[text] for text in dataset.loc[idx,'caption_pre_start_end'].split()]\n",
        "        for i in range(1, len(w2i)):\n",
        "            padded_sequence.append(w2i[:i])\n",
        "            next_words.append(w2i[i])\n",
        "        padded_partial_seq = sequence.pad_sequences(padded_sequence, max_caption_size, padding='post')\n",
        "\n",
        "        y = np.zeros((len(next_words), vocab_size), dtype=np.int32)\n",
        "\n",
        "        # One-Hot encoding\n",
        "        for idx, next_word in enumerate(next_words):\n",
        "            y[idx, next_word] = 1\n",
        "\n",
        "        # Agregamos la secuencia que tiene el padding a la lista total (son 5 por cada imagen)\n",
        "        # la dimensión es de (total_images, total_captions, max_caption_size)\n",
        "        all_padded_sequences.append(padded_partial_seq)\n",
        "        all_subsequence_words.append(y)\n",
        "    \n",
        "    return (np.array(all_padded_sequences), np.array(all_subsequence_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF7plZ1NssC-",
        "colab_type": "text"
      },
      "source": [
        "### 6. Dividimos el dataset en Train, Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwUJFw_FssC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['caption_pre'] = dataset['caption'].apply(preprocess_captions)\n",
        "dataset['caption_pre_start_end'] = dataset['caption_pre'].apply(add_start_end_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc6mErbQssDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size       = 6000\n",
        "#test_size        = 1591\n",
        "\n",
        "all_images = list(images_map.keys())\n",
        "# Cargamos la data de entrenamiento (Train = 6500)\n",
        "train_images_ids = deepcopy(all_images[:train_size])\n",
        "train_dataset = dataset[dataset['image'].isin(train_images_ids)].copy()\n",
        "\n",
        "# Cargamos la data de test (Test = 1591)\n",
        "test_images_ids  = deepcopy(all_images[train_size:])\n",
        "test_dataset = dataset[dataset['image'].isin(train_dataset)].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcB7f-1ossDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Almacenamos todas las oraciones en una lista\n",
        "captions = train_dataset['caption_pre_start_end'].tolist()\n",
        "\n",
        "# Obtenemos el vocabulario\n",
        "vocabulary = get_vocabulary(captions)\n",
        "\n",
        "# Vectorizamos a Word2Index and Index2Word\n",
        "word_to_index = create_word_to_index(vocabulary)\n",
        "index_to_word = create_index_to_word(vocabulary)\n",
        "\n",
        "# Obtenemos el tamaño del vocabulario\n",
        "vocab_size = get_vocabulary_size(vocabulary)\n",
        "\n",
        "# Obtenemos la oración más larga para hacer un padding\n",
        "max_caption_size = get_max_caption_size(captions)\n",
        "\n",
        "# Creamos la sencuencia de los captions\n",
        "padded_sequences, subsequence_words = create_sequences(train_dataset[:train_size], word_to_index, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esws019fssDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verificamos tamaño de vocabulario y cantidad de imagenes\n",
        "print(\"Maxima dimension de caption:\", max_caption_size)\n",
        "print(\"Total de imágenes:\", len(train_images_ids))\n",
        "print(\"Tamaño de vocabulario original:\", vocab_size)\n",
        "print(\"Total de secuencias:\", len(padded_sequences))\n",
        "print(\"Total de subsecuencias:\", len(subsequence_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApvaMGjjssDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do = True\n",
        "total_images = 3000\n",
        "if do:\n",
        "    captions = np.vstack(padded_sequences[:total_images])\n",
        "    next_words = np.vstack(subsequence_words[:total_images])\n",
        "    #np.save(\"precomputed/captions_resnet.npy\", captions)\n",
        "    #np.save(\"precomputed/next_words_resnet.npy\", next_words)\n",
        "    \n",
        "    print(captions.shape)\n",
        "    print(next_words.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSR-RZeUssDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with open('precomputed/encoded_images.p', 'rb') as f:\n",
        "with open('/gdrive/My Drive/dataset_imagecaption/Jorge_Rodriguez/resnet50/precomputed/encoded_images.p', 'rb') as f:\n",
        "    all_features_map = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXLTW-GIssDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do:    \n",
        "    # Lo que hacemos es tener una lista de cada imagen en el dataset, debido\n",
        "    # a que tenemos 5 descripciones la lista va a tener 5 elementos repetidos por imagen\n",
        "    # entonces como hemos usado 6,000 para entrenamiento, nuestro resultado final debe ser\n",
        "    # de 30,000 registros\n",
        "\n",
        "    train_features_map = []\n",
        "    for image_id in train_dataset['image']:\n",
        "        train_features_map.append(list(all_features_map[image_id]))\n",
        "\n",
        "    train_features_map = np.asarray(train_features_map)\n",
        "    print(train_features_map.shape)\n",
        "\n",
        "    # Ahora como cada secuencia tiene un tamaño distinto, lo que se hace\n",
        "    # es que por cada imagen (teniendo en cuenta que son 5 repetidos)\n",
        "    # obtener las secuencias y agregarlas al arreglo train_images\n",
        "    # lo que obtendremos al final es una mapeo de [imagenes por cada secuencia]\n",
        "\n",
        "    train_images = []\n",
        "    for ix in range(total_images):\n",
        "        for iy in range(len(padded_sequences[ix])):\n",
        "            train_images.append(train_features_map[ix])\n",
        "\n",
        "    train_images = np.asarray(train_images)\n",
        "\n",
        "    #np.save(\"precomputed/images_train_resnet.npy\", train_images)\n",
        "\n",
        "    print(train_images.shape)\n",
        "\n",
        "    # Al igual que los features map por cada imagen, ahora tenemos que\n",
        "    # obtener la misma cantidad de codigos de las imagenes por secuencia\n",
        "    train_images_names = []\n",
        "    for ix in range(total_images):\n",
        "        for iy in range(len(padded_sequences[ix])):\n",
        "            train_images_names.append(train_dataset.loc[ix, 'image'])\n",
        "\n",
        "    train_images_names = np.asarray(train_images_names)\n",
        "\n",
        "    #np.save('precomputed/image_names_train_resnet.npy', train_images_names)\n",
        "\n",
        "    print(train_images_names.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmP2YPWb81Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train_features_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrfbQnwZssDX",
        "colab_type": "text"
      },
      "source": [
        "### 7. Cargamos todos los archivos generados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ZcvcLjssDj",
        "colab_type": "text"
      },
      "source": [
        "### 8. Creamos el modelo (Encoder - Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW0I2Fg1ssDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI6H9-qsssDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Sequential([\n",
        "    Dense(embedding_size, input_shape=(2048,)),\n",
        "    Activation('relu'),\n",
        "    RepeatVector(max_caption_size)])\n",
        "\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BZCK9VPssDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_caption_size),\n",
        "    LSTM(256, return_sequences=True),\n",
        "    TimeDistributed(Dense(embedding_size))\n",
        "])\n",
        "\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La753oIAssDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(encoder, to_file='encoder.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQs845zqssDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(decoder, to_file='decoder.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y_Fk3hFssD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unimos el encoder y el decoder\n",
        "encoder_decoder = Concatenate()([encoder.output, decoder.output])\n",
        "x = LSTM(128, return_sequences=True)(encoder_decoder)\n",
        "x = LSTM(512, return_sequences=False)(x)\n",
        "x = Dense(vocab_size)(x)\n",
        "out = Activation('softmax')(x)\n",
        "\n",
        "model = Model(inputs=[encoder.input, decoder.input], outputs=out)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKceCCmIssD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(model, to_file='decoder_decoder_model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHudrrg6ssD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrenamiento del model\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "history = model.fit([train_images, captions], next_words, batch_size=batch_size, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey1ykl8O_oTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('model_weights_resnet50.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cOJ582pBJgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_vector(resnet, path, image, target_size=(224,224,3)):\n",
        "    img = preprocess_images_resnet(path,image, target_size=target_size)\n",
        "    return resnet.predict(img).reshape(2048)\n",
        "\n",
        "def predict_image_caption(model, feature_vector, max_caption_size):\n",
        "    words = ['<start>']\n",
        "    word_pred = ''\n",
        "\n",
        "    while word_pred != '<end>' or len(words) > max_caption_size:\n",
        "        w2i = [word_to_index[word] for word in words]\n",
        "        w2i = sequence.pad_sequences([w2i], maxlen=max_caption_size, padding='post')\n",
        "        preds = model.predict([np.array([feature_vector]), np.array(w2i) ])\n",
        "        word_pred = index_to_word[np.argmax(preds[0])]\n",
        "        words.append(word_pred)\n",
        "    return ' '.join(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrGSjG3uBUw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_vector = get_feature_vector(model_resnet,'/content/', 'giraffe.jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjJsXDoDE6wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_image_caption(model, feature_vector, max_caption_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLOdpNx5ssD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrenamiento del modeloI’m just \n",
        "# Crear el Decoder (LSTM) (Listo)\n",
        "# Crear generar texto (nueva imagen)\n",
        "# Evalución (BLEU)\n",
        "# * Modelo de Attention"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}